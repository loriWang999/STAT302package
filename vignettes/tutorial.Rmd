---
title: "STAT302package Tutorial"
author: "Lori Wang & Andrew Lee"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{STAT302package Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, warning=FALSE}
library(STAT302package)
library(magrittr)
library(dplyr)
library(kableExtra)
library(ggplot2)
```

## Part 1: R Package Development

### Introduction:
This package features many statistical functions developed by me for the STAT 302 class at the University of Washington. It can be installed via the following code:
```{r, eval = FALSE}
devtools::install_github("loriWang999/STAT302package", build_vignette = TRUE, build_opts = c())
```
### 1. A tutorial for my_t.test
  In this section, we would use $lifeExp$ data from $my\_gapminder$ to show all hypothesis conditions (two.side, less, greater) using my_t.test function.
  
$$H_0: \mu = 60$$
$$H_a: \mu \neq 60$$

$$\alpha = 0.05$$



```{r}
mu <- 60
my_t.test(my_gapminder$lifeExp, "two.sided", mu)

```
This is a two-tail my_t.test with null hypothesis $\mu$ = 60, and alternative hypothesis that $H_a$ is not equal to 60. And $p\_value$ of this test equals to 0.093 which is greater than $\alpha$. Therefore, we fail to reject the null hypothesis. 

$$H_0: \mu = 60$$
$$H_a: \mu > 60$$
$$\alpha = 0.05$$

```{r}
my_t.test(my_gapminder$lifeExp, "greater", mu)
```
This is an one-tail my_t.test with null hypothesis $\mu$ = 60, and alternative hypothesis that $H_a$ is bigger than 60. And $p\_value$ of this test equals to 0.95 which is greater than $\alpha$. Therefore, we fail to reject the null hypothesis. 

$$H_0: \mu = 60$$

$$H_a: \mu < 60$$
$$\alpha = 0.05$$

```{r}
my_t.test(my_gapminder$lifeExp, "less", mu)
```
This is an one-tail my_t.test with null hypothesis $\mu$ = 60, and alternative hypothesis that $H_a$ is less than 60. And $p\_value$ of this test equals to 0.046 which is less than $\alpha$. Therefore, we have enough evident to reject the null hypothesis. 

### 2. A tutorial for my_lm
In this section, we would use also use $my\_gapminder$ data to show the linear regression model with the function my_lm. For this regression, $lifeExp$ is response variable and $gdpPercap$ and $continent$ as explanatory variables.

```{r, warning=FALSE}
my_lm_model <- my_lm(lifeExp ~ gdpPercap + continent, my_gapminder)
kable_styling(kable(my_lm_model))
co_lifeExp <- my_lm_model[2,1]
co_lifeExp 
```
From the $coefficient$ of $gdpPercap$ above, we know that $lifeExp$ has a positive linear relationship with $gdpPercap$. Also, this value could tell us the expected difference in $lifeExp$ between two observations differing by one unit in $gdpPercap$, with all other covariates identical.


Then we set a two.sided hypothesis test of $gdpPercap$ coefficient.
$$H_0: gdpPercap\_co = 0$$

$$H_a: gdpPercap\_co \neq 0$$
$$\alpha = 0.05$$
```{r, warning=FALSE}
my_lm_p <- my_lm_model[2,4]
my_lm_p < 0.05
```

We found that the $p\_value$ is much smaller than 0.05. Therefore, we have enough evident to reject the null hypothesis. 

```{r}
# calculate y^hat
my_co <- as.matrix(my_lm_model[,1])
x <- model.matrix(lifeExp ~ gdpPercap + continent, my_gapminder)
y_hat <- x %*% my_co
my_df <- data.frame(actual = my_gapminder$lifeExp,
                    fitted = y_hat, continent = my_gapminder$continent)
ggplot(my_df, aes(x = fitted, y = actual, color = continent)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, col = "black", lty = 2) + 
  theme_bw(base_size = 10) +
  labs(x = "Fitted values", y = "Actual values", title = "Actual vs. Fitted") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 14))
```



This graph interpret the Actual vs. Fitted plot. We could find that our fitted values are not completely follow the actual values. For example, for Europe and Oceania, the fitted values are follow the pattern of actual values. However, for other countries, our fitted values are bad with actual values, especially for Africa. 

### 4. my_knn_cv Demonstration
This function performs a k-nearest neighbors non-parametric prediction algorithm. It splits a provided dataset into training and testing segments based on
the number of "neighbors," or the distance from the training data, desired. It will predict new cases provided in the testing data given the training data.
We want to predict \code{species} using covariates \code{bill_length_mm}, \code{bill_depth_mm}, \code{flipper_length_mm}, \code{body_mass_g} with \code{k_cv = 5}.
Inputs:
- \code{train}: input data frame
- \code{cl}: true class value of the training data
- \code{k_nn}: integer representing the number of neighbors
- \code{k_cv}: integer representing the number of folds

```{r}

# import my_penguins data
data(my_penguins)

# delete rows with NA data
penguins <- na.omit(my_penguins)

# create x and y matrices
train <- penguins[3:6]
cl <- penguins %>% pull(species)

# Initiate empty lists to store training and CV misclassfication rates
training_error = rep(NA, length(10))
cv_error = rep(NA, length(10))

# Iterate different fits from k_nn of 1 to 10
for (n in 1:10){
  results <- my_knn_cv(train, cl, n, 5)
  training_error[n] <- sum(penguins$species != results[[1]]) / nrow(penguins)
  cv_error[n] <- results[[2]]
}

# Organize the errors into a table
err_data <- data.frame("k_nn" = c(1:10),
                       "cv_error" = cv_error,
                       "training_error" = training_error)

kable_styling(kable(err_data))
```


Based on both the training and CV misclassification rates, a fold of 1 would be ideal since it has the minimum value for both. In practice, it is always the best to use the model with the least CV error, so a k_nn of 1 would also be chosen for practice.

### 5. my_rf_cv Demonstration
Instead of classifying based on neighboring data points in the training data,
the random forest algorithm classify based on splits in the parameters.
We want to predict \code{body_mass_g} using covariates \code{bill_length_mm}, \code{bill_depth_mm}, \code{flipper_length_mm}.
Inputs:
- \code{k}: number of folds
```{r}
# remove NA data in the data set
penguins <- na.omit(penguins)

# create a list to label the number of folds
folds <- c(rep("2", 30), rep("5", 30), rep("10", 30))

# create empty lists to store MSE
MSE_2 <- rep(NA, length(30))
MSE_5 <- rep(NA, length(30))
MSE_10 <- rep(NA, length(30))

# fill MSE data
for (k in 1:30){
  MSE_2[k] <- my_rf_cv(2)
  MSE_5[k] <- my_rf_cv(5)
  MSE_10[k] <- my_rf_cv(10)
}

# combine the label and data into one data frame
MSE_data <- data.frame(names = folds,
                       MSE = c(MSE_2, MSE_5, MSE_10))
```
```{r}
# plot the data
ggplot(data = MSE_data,
       aes(x = names, MSE)) +
  geom_boxplot(fill = "lightblue") +
  theme_bw(base_size = 20) +
  labs(title = "MSE According to Number of Folds",
       x = "Number of Folds",
       y = "MSE") +
  theme(plot.title =
          element_text(hjust = 0.5),
        text = element_text(size = 12))  
```
```{r}
MSE_table <- data.frame("folds" = c(2, 5, 10),
                        "mean" = c(mean(MSE_2), mean(MSE_5), mean(MSE_10)),
                        "std_deviation" = c(sd(MSE_2), sd(MSE_5), sd(MSE_10)))
kable_styling(kable(MSE_table))
```
In general, a fold of 2 has the widest standard deviation, while a fold of 10 gives the lowest mean MSE. THis could be the case because having 10 folds allows for the algorithm to best predict the body weight of the penguins in the dataset.


